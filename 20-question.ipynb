{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd443c3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-08T04:45:21.261118Z",
     "iopub.status.busy": "2024-08-08T04:45:21.260802Z",
     "iopub.status.idle": "2024-08-08T04:45:21.288671Z",
     "shell.execute_reply": "2024-08-08T04:45:21.287990Z"
    },
    "papermill": {
     "duration": 0.033972,
     "end_time": "2024-08-08T04:45:21.290602",
     "exception": false,
     "start_time": "2024-08-08T04:45:21.256630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36af51d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T04:45:21.297628Z",
     "iopub.status.busy": "2024-08-08T04:45:21.297354Z",
     "iopub.status.idle": "2024-08-08T04:45:21.307391Z",
     "shell.execute_reply": "2024-08-08T04:45:21.306471Z"
    },
    "papermill": {
     "duration": 0.016012,
     "end_time": "2024-08-08T04:45:21.309403",
     "exception": false,
     "start_time": "2024-08-08T04:45:21.293391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")\n",
    "else:\n",
    "    model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n",
    "\n",
    "\n",
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()\n",
    "    start_gen = inp_ids.input_ids.shape[1]\n",
    "    out_ids = out_ids[start_gen:]\n",
    "    if id_eot in out_ids:\n",
    "        stop = out_ids.tolist().index(id_eot)\n",
    "        out = tokenizer.decode(out_ids[:stop])\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)\n",
    "    return out\n",
    "    \n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            #launch the asker role\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            #launch the answerer role\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"   \n",
    "            if (\"yes\" not in output.lower() and \"no\" not in output.lower()):\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            #launch the guesser role\n",
    "            output = self.asker(obs)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def asker(self, obs):\n",
    "        sys_prompt = \"\"\"You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
    "        the user is going to think of a word, it can be only one of the following 2 categories:\n",
    "        1. a place\n",
    "        2. things\n",
    "        So focus your area of search on these options. and give smart questions that narrows down the search space\\n\"\"\"\n",
    "    \n",
    "        if obs.turnType ==\"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"your role is to find the word by asking him up to 20 questions, your questions to be valid must have only a 'yes' or 'no' answer.\n",
    "            to help you, here's an example of how it should work assuming that the keyword is piano:\n",
    "            example:\n",
    "            <you: is it a thing?\n",
    "            user: yes\n",
    "            you: is it used in a home?\n",
    "            user: yes\n",
    "            you: is it furniture?\n",
    "            user: no\n",
    "            you: is it related to entertainment?\n",
    "            user: yes\n",
    "            you: is it an electronic device?\n",
    "            user: no\n",
    "            you: is it a musical instrument?\n",
    "            user: yes\n",
    "            you: does it have keys?\n",
    "            user: yes\n",
    "            you: is it a piano?\n",
    "            user: yes.>\n",
    "\n",
    "            the user has chosen the word, ask your first question!\n",
    "            please be short and not verbose, give only one question, no extra word!\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            if len(obs.questions)>=1:\n",
    "                for q, a in zip(obs.questions, obs.answers):\n",
    "                    chat_template += f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                    chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                conv += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"so far, the current state of the game is as following:\\n{conv}\n",
    "            based on the conversation, can you guess the word, please give only the word, no verbosity around\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                \n",
    "        output = generate_answer(chat_template)        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "    def answerer(self, obs):\n",
    "        sys_prompt = f\"\"\"you are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
    "        the role of the user is to guess the word by asking you up to 20 questions, your answers to be valid must be a 'yes' or 'no', any other answer is invalid and you lose the game.\n",
    "        Know that the user will always guess a word belonging to one of the following 3 categories:\n",
    "        1. a place\n",
    "        2. things\n",
    "        so make sure you understand the user's question and you understand the keyword you're playig on.\n",
    "        for now the word that the user should guess is: \"{obs.keyword}\", it is of category \"{obs.category}\",\n",
    "        to help you, here's an example of how it should work assuming that the keyword is piano in the for a category \"things\":\n",
    "        example:\n",
    "        <you: is it a thing?\n",
    "        user: yes\n",
    "        you: is it used in a home?\n",
    "        user: yes\n",
    "        you: is it furniture?\n",
    "        user: no\n",
    "        you: is it related to entertainment?\n",
    "        user: yes\n",
    "        you: is it an electronic device?\n",
    "        user: no\n",
    "        you: is it a musical instrument?\n",
    "        user: yes\n",
    "        you: does it have keys?\n",
    "        user: yes\n",
    "        you: is it a piano?\n",
    "        user: yes.>\"\"\"\n",
    "        \n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.answers)>=1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    if obs.turnType ==\"ask\":\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    if response == None or len(response)<=1:\n",
    "        response = \"yes\"\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c5688e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T04:45:21.315617Z",
     "iopub.status.busy": "2024-08-08T04:45:21.315159Z",
     "iopub.status.idle": "2024-08-08T04:45:27.968043Z",
     "shell.execute_reply": "2024-08-08T04:45:27.966865Z"
    },
    "papermill": {
     "duration": 6.658746,
     "end_time": "2024-08-08T04:45:27.970607",
     "exception": false,
     "start_time": "2024-08-08T04:45:21.311861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3de2f64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T04:45:27.977901Z",
     "iopub.status.busy": "2024-08-08T04:45:27.977581Z",
     "iopub.status.idle": "2024-08-08T04:51:08.839151Z",
     "shell.execute_reply": "2024-08-08T04:51:08.838130Z"
    },
    "papermill": {
     "duration": 340.867873,
     "end_time": "2024-08-08T04:51:08.841478",
     "exception": false,
     "start_time": "2024-08-08T04:45:27.973605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/input/llama-3/transformers/8b-chat-hf . -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de15ac",
   "metadata": {
    "papermill": {
     "duration": 0.029121,
     "end_time": "2024-08-08T04:51:08.899247",
     "exception": false,
     "start_time": "2024-08-08T04:51:08.870126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 351.039414,
   "end_time": "2024-08-08T04:51:09.248000",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-08T04:45:18.208586",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
